Using TensorFlow backend.
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.

2022-07-15 13:12:16.254832: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2022-07-15 13:12:16.271887: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2022-07-15 13:12:16.724064: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fdcd183380 executing computations on platform CUDA. Devices:
2022-07-15 13:12:16.724145: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Graphics Device, Compute Capability 7.0
2022-07-15 13:12:16.728295: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-07-15 13:12:16.731349: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fdcd2c4180 executing computations on platform Host. Devices:
2022-07-15 13:12:16.731427: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-07-15 13:12:16.733089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Graphics Device major: 7 minor: 0 memoryClockRate(GHz): 1.597
pciBusID: 0000:3b:00.0
2022-07-15 13:12:16.733906: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2022-07-15 13:12:16.736059: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2022-07-15 13:12:16.738077: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2022-07-15 13:12:16.738534: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2022-07-15 13:12:16.741245: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2022-07-15 13:12:16.743404: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2022-07-15 13:12:16.749639: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2022-07-15 13:12:16.753001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2022-07-15 13:12:16.753116: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2022-07-15 13:12:16.755001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-07-15 13:12:16.755038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2022-07-15 13:12:16.755069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2022-07-15 13:12:16.758184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30458 MB memory) -> physical GPU (device: 0, name: Graphics Device, pci bus id: 0000:3b:00.0, compute capability: 7.0)
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        (None, None)         0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      (None, None)         0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     multiple             9216000     Input-Token[0][0]                
                                                                 MLM-Norm[0][0]                   
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Attention-UniLM-Mask (Lambda)   (None, 1, None, None 0           Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Rotary-Position (Sinu (None, None, 64)     0           Embedding-Dropout[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
                                                                 Attention-UniLM-Mask[0][0]       
                                                                 Embedding-Rotary-Position[0][0]  
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Attention-UniLM-Mask[0][0]       
                                                                 Embedding-Rotary-Position[0][0]  
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Attention-UniLM-Mask[0][0]       
                                                                 Embedding-Rotary-Position[0][0]  
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Attention-UniLM-Mask[0][0]       
                                                                 Embedding-Rotary-Position[0][0]  
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Attention-UniLM-Mask[0][0]       
                                                                 Embedding-Rotary-Position[0][0]  
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Attention-UniLM-Mask[0][0]       
                                                                 Embedding-Rotary-Position[0][0]  
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Attention-UniLM-Mask[0][0]       
                                                                 Embedding-Rotary-Position[0][0]  
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Attention-UniLM-Mask[0][0]       
                                                                 Embedding-Rotary-Position[0][0]  
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Attention-UniLM-Mask[0][0]       
                                                                 Embedding-Rotary-Position[0][0]  
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Attention-UniLM-Mask[0][0]       
                                                                 Embedding-Rotary-Position[0][0]  
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Attention-UniLM-Mask[0][0]       
                                                                 Embedding-Rotary-Position[0][0]  
__________________________________________________________________________________________________WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-07-15 13:13:12.071794: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0

Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Attention-UniLM-Mask[0][0]       
                                                                 Embedding-Rotary-Position[0][0]  
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
MLM-Dense (Dense)               (None, None, 768)    590592      Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
MLM-Norm (LayerNormalization)   (None, None, 768)    1536        MLM-Dense[0][0]                  
__________________________________________________________________________________________________
MLM-Bias (ScaleOffset)          (None, None, 12000)  12000       Embedding-Token[1][0]            
__________________________________________________________________________________________________
MLM-Activation (Activation)     (None, None, 12000)  0           MLM-Bias[0][0]                   
__________________________________________________________________________________________________
cross_entropy_1 (CrossEntropy)  (None, None, 12000)  0           Input-Token[0][0]                
                                                                 Input-Segment[0][0]              
                                                                 MLM-Activation[0][0]             
==================================================================================================
Total params: 94,877,664
Trainable params: 94,877,664
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/200
 - 131s - loss: 2.3976
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听了就很悲伤，孔子说：“苛政猛于虎，我曾经担心过是这样的，现在我以蒋氏的观点去看他，还是很信任他。
valid_data: {'rouge-1': 0.5842451028453086, 'rouge-2': 0.31620546417647694, 'rouge-l': 0.5568194211298347, 'bleu': 0.21229037256037345, 'best_bleu': 0.21229037256037345}
Epoch 2/200
 - 103s - loss: 2.0166
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听到这件事后越来越悲伤，孔子说：“苛政猛于虎啊！”我曾经怀疑过这件事，现在以蒋氏为观点，还是相信。
valid_data: {'rouge-1': 0.5985157071240249, 'rouge-2': 0.3366119048575613, 'rouge-l': 0.5710939585298885, 'bleu': 0.23048478239418196, 'best_bleu': 0.23048478239418196}
Epoch 3/200
 - 105s - loss: 1.7989
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听了更加悲伤，孔子说：“苛政猛于虎啊！”我曾经怀疑过这件事，现在用蒋氏来观察，还是相信的。
valid_data: {'rouge-1': 0.6002804577271128, 'rouge-2': 0.3444617290376822, 'rouge-l': 0.5738237540949773, 'bleu': 0.24237161040810848, 'best_bleu': 0.24237161040810848}
Epoch 4/200
 - 109s - loss: 1.6166
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听了更加悲哀，孔子说：“苛政真是猛于虎啊！”我曾经怀疑过这样的话，现在以蒋氏为观点，还是相信的。
valid_data: {'rouge-1': 0.6097726409999298, 'rouge-2': 0.35407884349298796, 'rouge-l': 0.5822401965756082, 'bleu': 0.24794862534222042, 'best_bleu': 0.24794862534222042}
Epoch 5/200
 - 107s - loss: 1.4571
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听了越来越悲伤，孔子说：“苛政真是猛如虎啊！”我曾经怀疑过这个问题，现在以蒋氏为观点，还是相信的。
valid_data: {'rouge-1': 0.6109039544610181, 'rouge-2': 0.3610909512745556, 'rouge-l': 0.5839454463543825, 'bleu': 0.2603792441329774, 'best_bleu': 0.2603792441329774}
Epoch 6/200
 - 107s - loss: 1.3174
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听了更加悲伤，孔子说：“苛政猛于虎啊！”我曾经怀疑过这个问题，现在以蒋氏为观点，还是相信。
valid_data: {'rouge-1': 0.6175167982622757, 'rouge-2': 0.3696226750105018, 'rouge-l': 0.5919266617963694, 'bleu': 0.2665891817415456, 'best_bleu': 0.2665891817415456}
Epoch 7/200
 - 113s - loss: 1.1842
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听了越来越悲哀，孔子说：“苛政真是猛于虎啊！”我曾经怀疑过这个问题，现在以蒋氏为观点来看，还是相信的。
valid_data: {'rouge-1': 0.6177767592570961, 'rouge-2': 0.37264596872612116, 'rouge-l': 0.592024509721648, 'bleu': 0.2724449409283073, 'best_bleu': 0.2724449409283073}
Epoch 8/200
 - 111s - loss: 1.0588
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听了更加悲伤，孔子说：“苛政恐怕是猛虎一样的！”我曾经怀疑过这种情况，现在以蒋氏为观察对待，还是相信。
Early stop count 1/5
valid_data: {'rouge-1': 0.6161581783996408, 'rouge-2': 0.36984633929166144, 'rouge-l': 0.5898907175822461, 'bleu': 0.2690527022390898, 'best_bleu': 0.2724449409283073}
Epoch 9/200
 - 120s - loss: 0.9478
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听了愈发悲伤，孔子说：“苛政恐怕要比虎更猛一些！”我曾经怀疑过这个问题，现在以蒋氏为人看他，还是相信他。
valid_data: {'rouge-1': 0.6210424916563138, 'rouge-2': 0.37672407375152994, 'rouge-l': 0.5935051121801057, 'bleu': 0.2776168098486086, 'best_bleu': 0.2776168098486086}
Epoch 10/200
 - 123s - loss: 0.8437
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听了愈发悲伤，孔子说：“苛政恐怕是猛于虎了！”我曾经怀疑过这个问题，现在以蒋氏为观察者，仍然相信它。
valid_data: {'rouge-1': 0.6225824101796277, 'rouge-2': 0.380172878558057, 'rouge-l': 0.5958044539957726, 'bleu': 0.28380498594411563, 'best_bleu': 0.28380498594411563}
Epoch 11/200
 - 125s - loss: 0.7533
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听了愈发悲伤，孔子说：“苛政令人像虎一样厉害！”我曾经怀疑过这种情况，现在以蒋氏为典型来看待它，还是相信的。
Early stop count 1/5
valid_data: {'rouge-1': 0.6184437189207478, 'rouge-2': 0.37700658746138765, 'rouge-l': 0.5907459557102203, 'bleu': 0.2831530877625486, 'best_bleu': 0.28380498594411563}
Epoch 12/200
 - 155s - loss: 0.6663
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听了愈发悲伤，孔子说：“苛政恐怕是猛于虎了！”我曾经怀疑过这种情况，现在以蒋氏为观察者，还是相信他。
valid_data: {'rouge-1': 0.619199948833996, 'rouge-2': 0.37903291059428584, 'rouge-l': 0.5919584911382767, 'bleu': 0.2845738200585394, 'best_bleu': 0.2845738200585394}
Epoch 13/200
 - 156s - loss: 0.5860
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听说这件事，越来越悲伤，孔子说：“苛政令人类都像猛虎一样厉害！”我曾经怀疑过这件事，现在以蒋氏为典范来观察，还是相信。
valid_data: {'rouge-1': 0.6199090694628864, 'rouge-2': 0.38263654480878917, 'rouge-l': 0.5935327150098506, 'bleu': 0.287574661830397, 'best_bleu': 0.287574661830397}
Epoch 14/200
 - 150s - loss: 0.5156
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听了愈发悲伤，孔子说：“苛政令人可以比虎更猛呀！”我曾经怀疑过这种情况，现在以蒋氏为典型来看待，还是相信的。
valid_data: {'rouge-1': 0.6255509977218138, 'rouge-2': 0.3894871383863648, 'rouge-l': 0.5982982027236791, 'bleu': 0.29535379706452725, 'best_bleu': 0.29535379706452725}
Epoch 15/200
 - 171s - loss: 0.4538
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听说苛政是猛于虎的，孔子说：“苛政恐怕要比虎更猛。”我以前曾经怀疑过这种情况，现在用蒋氏的观察，还是相信。
valid_data: {'rouge-1': 0.6275543058738465, 'rouge-2': 0.39369665965872785, 'rouge-l': 0.6016465819450374, 'bleu': 0.29928973020393884, 'best_bleu': 0.29928973020393884}
Epoch 16/200
 - 165s - loss: 0.3958
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听说这件事，越是严苛的政治越是猛虎！”孔子说：“我以前怀疑这件事，现在用蒋氏来看他，还是信实。
valid_data: {'rouge-1': 0.6306268932539307, 'rouge-2': 0.3974006995081798, 'rouge-l': 0.6035485074108413, 'bleu': 0.302013868612948, 'best_bleu': 0.302013868612948}
Epoch 17/200
 - 165s - loss: 0.3499
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听了愈发悲伤，孔子说：“苛政恐怕要比虎更严厉了！”我以前曾经怀疑过这种情况，现在用蒋氏来看他，还是相信。
Early stop count 1/5
valid_data: {'rouge-1': 0.6290657328781559, 'rouge-2': 0.39506414306256155, 'rouge-l': 0.6025995505499072, 'bleu': 0.3007143659473205, 'best_bleu': 0.302013868612948}
Epoch 18/200
 - 175s - loss: 0.3085
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听了愈是悲伤，孔子说：“苛政恐怕要比虎更严厉了！”我以前曾经怀疑过这种情况，现在用蒋氏来看他，还是相信。
valid_data: {'rouge-1': 0.6303828178601305, 'rouge-2': 0.39577575659896097, 'rouge-l': 0.6026838646597105, 'bleu': 0.302209184421507, 'best_bleu': 0.302209184421507}
Epoch 19/200
 - 173s - loss: 0.2680
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听说苛政是猛虎的一种表现！”孔子说：“我以前曾经怀疑这种情况，现在以蒋氏为看法，还能相信吗？”
valid_data: {'rouge-1': 0.6363762686521863, 'rouge-2': 0.40412360728942653, 'rouge-l': 0.6093350485983646, 'bleu': 0.30967894791227446, 'best_bleu': 0.30967894791227446}
Epoch 20/200
 - 171s - loss: 0.2358
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听说了，越来越悲伤，孔子说：“苛政令人可以比虎更猛！”我以前曾经怀疑过此事，现在用蒋氏来看他，还是相信。
Early stop count 1/5
valid_data: {'rouge-1': 0.6258369440749442, 'rouge-2': 0.39284074698758115, 'rouge-l': 0.5984080720075076, 'bleu': 0.3006327946125387, 'best_bleu': 0.30967894791227446}
Epoch 21/200
 - 172s - loss: 0.2084
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听说这件事越是悲伤越是孔子说过：“苛政令人猛如虎啊！”我以前怀疑这件事，现在以蒋氏看他，还是相信。
Early stop count 2/5
valid_data: {'rouge-1': 0.6300300506479558, 'rouge-2': 0.4007490875123016, 'rouge-l': 0.6043787387664002, 'bleu': 0.30714705045269014, 'best_bleu': 0.30967894791227446}
Epoch 22/200
 - 169s - loss: 0.1839
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听说过这种说法，愈是悲伤，孔子说：“苛政的政治要比虎更猛！”我以前曾经怀疑过这种想法，现在用蒋氏看他，还是信实。
Early stop count 3/5
valid_data: {'rouge-1': 0.6299227205709066, 'rouge-2': 0.3968292165880056, 'rouge-l': 0.6035956974932905, 'bleu': 0.30462208275681096, 'best_bleu': 0.30967894791227446}
Epoch 23/200
 - 166s - loss: 0.1638
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听说过这种情况，愈是悲伤，孔子说：“苛政是猛于虎的！”我以前曾经怀疑过这种情况，现在用蒋氏来观察，还是相信。
Early stop count 4/5
valid_data: {'rouge-1': 0.6289127410737323, 'rouge-2': 0.3993414704843619, 'rouge-l': 0.6019268504613517, 'bleu': 0.30784993077869177, 'best_bleu': 0.30967894791227446}
Epoch 24/200
 - 165s - loss: 0.1475
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听说过这种情况，愈是悲伤，孔子说：“苛政是猛于虎的啊！”我以前曾经怀疑过这种情况，现在用蒋氏来看他，还是很可信。
valid_data: {'rouge-1': 0.6354841784538214, 'rouge-2': 0.4083275732559526, 'rouge-l': 0.6094241409208192, 'bleu': 0.3149648896381498, 'best_bleu': 0.3149648896381498}
Epoch 25/200
 - 165s - loss: 0.1325
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听说了，越是严苛的政治越猛，像虎一样！”孔子说：“我以前曾经怀疑这个问题，现在用蒋氏来看他，还是相信。
Early stop count 1/5
valid_data: {'rouge-1': 0.6297695210541802, 'rouge-2': 0.4003098830782893, 'rouge-l': 0.602590682469053, 'bleu': 0.3082482969830215, 'best_bleu': 0.3149648896381498}
Epoch 26/200
 - 168s - loss: 0.1183
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听说苛政是猛于虎的，孔子说：“苛政恐怕是猛于虎吧！”我以前曾经怀疑过这种情况，现在用蒋氏来看他，还是相信。
Early stop count 2/5
valid_data: {'rouge-1': 0.6308378294798038, 'rouge-2': 0.400053036750899, 'rouge-l': 0.6040730337830039, 'bleu': 0.3068059184122339, 'best_bleu': 0.3149648896381498}
Epoch 27/200
 - 164s - loss: 0.1082
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听说苛政是猛于虎的，孔子说：“苛政是猛于虎的！”我以前曾经怀疑过这种情况，现在用蒋氏来看他，还能相信吗？”
Early stop count 3/5
valid_data: {'rouge-1': 0.6293172902775276, 'rouge-2': 0.3994300190051216, 'rouge-l': 0.6036114878535116, 'bleu': 0.3077445379078206, 'best_bleu': 0.3149648896381498}
Epoch 28/200
 - 164s - loss: 0.0993
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听说苛政是猛虎啊！”孔子说：“苛政恐怕要比虎更猛！”我以前曾经怀疑过这种情况，现在用蒋氏来观察，还是信实。
Early stop count 4/5
valid_data: {'rouge-1': 0.6306212813424421, 'rouge-2': 0.40158040478131624, 'rouge-l': 0.6045151142206323, 'bleu': 0.310497963986462, 'best_bleu': 0.3149648896381498}
Epoch 29/200
 - 162s - loss: 0.0912
原文 余闻而愈悲，孔子曰：“苛政猛于虎也！”吾尝疑乎是，今以蒋氏观之，犹信。
译文 我听说苛政大于虎，孔子说：“苛政大于虎啊！”我曾经怀疑过这个问题，现在以蒋氏看他，还能相信。
Early stop count 5/5
Epoch 00028: early stopping THR.
valid_data: {'rouge-1': 0.6335207076843986, 'rouge-2': 0.40422218691714723, 'rouge-l': 0.6081162016522619, 'bleu': 0.31214009804785386, 'best_bleu': 0.3149648896381498}
